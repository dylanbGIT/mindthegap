{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Retreival\n",
    "\n",
    "I am working with several datasets in this project. Several are publicly available as flat files, while others I have had to construct myself from scraped sites. This notebook walks through all the data retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.common import get_filepath_or_buffer\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from requests_futures.sessions import FuturesSession\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import dill\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnstile and Fare Card Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dated_links(url, select):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, \"lxml\")\n",
    "    tags = soup.select(select)\n",
    "            \n",
    "    links = [(tag.attrs['href'], datetime.strptime(tag.contents[0], '%A, %B %d, %Y').date()) for tag in tags]\n",
    "    return links\n",
    "\n",
    "def filter_by_date(links, cutoff=datetime(2016,1,1).date()):\n",
    "    return list(filter(lambda x: x[1] >= cutoff, links))\n",
    "\n",
    "def dl_MTA_data(url,links,out):\n",
    "    basePage = re.match(r'(.+?/)\\w+.html', url).group(1)\n",
    "    pages = [basePage + link[0] for link in links]\n",
    "\n",
    "    session = FuturesSession(max_workers=3)\n",
    "    futures = [(session.get(page), page) for page in pages]\n",
    "\n",
    "    s = re.compile(r'.+/(\\w+\\.\\w{3})')\n",
    "    for future in tqdm_notebook(futures):\n",
    "        with open('../data/' + s.search(future[1]).group(1), 'w') as f:\n",
    "            f.write(future[0].result().text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have to inspect the HTML of the pages to make sure the correct files are getting retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# turnstile data\n",
    "url = 'http://web.mta.info/developers/turnstile.html'\n",
    "\n",
    "links = filter_by_date(get_dated_links(url, 'div.span-84 a'))\n",
    "dl_MTA_data(url, links, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# farecard data\n",
    "url = 'http://web.mta.info/developers/fare.html'\n",
    "\n",
    "links = filter_by_date(get_dated_links(url, 'div.span-19 a'))\n",
    "dl_MTA_data(url, links, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Census Data\n",
    "\n",
    "Unfortunately the API for American Community Survey is not well documented and I've had a hard time getting it to give me the data I want. I ultimately went to https://factfinder.census.gov/faces/nav/jsf/pages/download_center.xhtml and requested the 'DISABILITY CHARACTERISTICS' table from the 2015 ACS 5-year survey at the block level for New York, Bronx, Kings, and Queens counties. I've joined these 4 tables into one, which I'll be using in the other notebooks, and which I've included in the `data` directory in this repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station Geo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this feature will hopefully be merged into geopandas\n",
    "def read_geojson_url(url):\n",
    "    buffer, _, _ = get_filepath_or_buffer(url)\n",
    "    geojson = json.loads(buffer.read())\n",
    "    return gpd.GeoDataFrame.from_features(geojson['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'https://data.cityofnewyork.us/resource/kk4q-3rt2.geojson'\n",
    "\n",
    "read_geojson_url(url).to_pickle('../data/station_geodata.pkd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platform Accessibility Data\n",
    "There might be better ways to get it, but as far as I can tell, no one offers a convenient table of which platforms are accessible, so I scraped it from Wikipedia. Keep in mind some stations may contain both accessible and inaccessible platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wiki_data():\n",
    "    page = requests.get('https://en.wikipedia.org/wiki/List_of_accessible_New_York_City_Subway_stations')\n",
    "    soup = BeautifulSoup(page.text, \"lxml\")\n",
    "    tables = soup.select('table.wikitable')\n",
    "\n",
    "    allStations = []\n",
    "    allLines = []\n",
    "    for table in tables[:-1]:\n",
    "        tags = table.select('tr')[1:]\n",
    "        stations = [re.match(r'([^(]+)\\s', tag.select('th')[0].select('a')[0].attrs['title']).group(1) for tag in tags]\n",
    "        lines = [set([re.match(r'\\w+', item.attrs['title']).group() for item in tag.select('th')[1].select('a')]) for tag in tags]\n",
    "        allStations.extend(stations)\n",
    "        allLines.extend(lines)\n",
    "    \n",
    "    allStations = [item.replace('\\xe2\\x80\\x93',' - ') for item in allStations]\n",
    "    # since our DataFrame contains sets, pickle more convenient than CSV\n",
    "    return pd.DataFrame(list(zip(allStations,allLines)),columns=['stations','lines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_wiki_data().to_pickle('../data/platform_accesibility.pkd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platform connectivity data\n",
    "I want to know how platforms are connected so I can analyze the network graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_links(url, select):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, \"lxml\")\n",
    "    tags = soup.select(select)\n",
    "\n",
    "    links = [url + tag.attrs['href'] for tag in tags]\n",
    "    return links\n",
    "\n",
    "def parse_station(tag, selectors, pattern):\n",
    "    contents = []\n",
    "    for selector in selectors:\n",
    "        if tag.select(selector):\n",
    "            for selected in tag.select(selector):\n",
    "                contents.extend(selected.contents)\n",
    "            break\n",
    "    \n",
    "    for content in contents:\n",
    "        if isinstance(content, str):\n",
    "            match = pattern.match(content)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_line_data(links):\n",
    "    session = FuturesSession(max_workers=3)\n",
    "    futures = [(session.get(link), link) for link in links]\n",
    "    \n",
    "    selectors = ['span.emphasized strong', 'span.emphasized', 'strong', 'td']\n",
    "    stat_re = re.compile(r'[^\\w]*(.+\\w)')\n",
    "    \n",
    "    services = {}\n",
    "    for future in futures:\n",
    "        page = future[0].result()\n",
    "        soup = BeautifulSoup(page.text, \"lxml\")\n",
    "        tags = soup.findAll('table', {'summary' : re.compile(r'.*Subway Line Stops')})\n",
    "        tags = tags[0].select('tr[height=\"25\"]')\n",
    "        service = []\n",
    "        for tag in tags:\n",
    "            stat = parse_station(tag, selectors, stat_re)\n",
    "            trans = []\n",
    "            trans = re.findall(r'a href=\"(\\w+?).htm\"',str(tag.select('a')))\n",
    "            service.append((stat,trans))\n",
    "\n",
    "        services[re.match(r'.+/(\\w+).htm', future[1]).group(1)] = service\n",
    "    \n",
    "    return services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'http://web.mta.info/nyct/service/'\n",
    "select = 'div.roundCorners p a'\n",
    "\n",
    "links = get_links(url, select)\n",
    "\n",
    "with open('../data/subway_line_data.pkd', 'wb') as f:\n",
    "    dill.dump(get_line_data(links), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Travel time data\n",
    "The best way to determine which stations to prioritize for accessibility upgrades would be to measure the travel times from point to point and see how they change as stations are made accessible or inaccessible. While I haven't yet figured out a way to find travel times from point to point while including or omitting individual stations, a Google project called [Sidewalk Labs](https://www.sidewalklabs.com/) that allows querying travel times from a point to all points in NYC either with or without accessible stations. A friend of mine, [Micha Gorelick](https://github.com/mynameisfiber/), reverse engineered their API for [predicting impact of the L-train shutdown](https://github.com/mynameisfiber/lpocolypse) with a metric called [Earth mover's distance](https://en.wikipedia.org/wiki/Earth_mover%27s_distance). We can do the same for accessible stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
